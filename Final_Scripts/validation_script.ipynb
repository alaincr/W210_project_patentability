{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "sb_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    text_stem = ''\n",
    "    for word in text.lower().split():\n",
    "        if text_stem:\n",
    "            text_stem =  text_stem + ' ' + sb_stemmer.stem(word)\n",
    "        else:\n",
    "            text_stem = sb_stemmer.stem(word)\n",
    "    return text_stem\n",
    "\n",
    "\n",
    "def clean_stem(string):\n",
    "    stopwords = {'back', 'thru', 'eg', 'hereafter', 'too', 'part', 'which', 'will', 'be', 'thereupon', 'about', 'nevertheless', 'therein', 'through', 'we', 'among', 'in', 'then', 'former', 'via', 'below', 'whereafter', 'due', 'you', 'bill', 'forty', 'few', 'not', 'with', 'rather', 'next', 'nine', 'me', 'its', 'sometime', 'yours', 'who', 'whoever', 'down', 'some', 'such', 'thereafter', 'hasnt', 'fifteen', 'both', 'as', 'ever', 'could', 'find', 'hence', 'something', 'a', 'there', 'mostly', 'whereas', 'many', 'serious', 'can', 'indeed', 'afterwards', 'whenever', 'by', 'becomes', 'may', 'after', 'couldnt', 'seemed', 'anyhow', 'etc', 'might', 'already', 'no', 'please', 'them', 'myself', 'therefore', 'from', 'along', 'ltd', 'against', 'everywhere', 'amoungst', 'because', 'where', 'sixty', 'ie', 'although', 'sincere', 'move', 'seeming', 'or', 'wherever', 'inc', 'whatever', 'into', 'anywhere', 'around', 'nor', 'see', 'several', 'sometimes', 'for', 'interest', 'beyond', 'whether', 'detail', 'describe', 'moreover', 'nobody', 'whereupon', 're', 'without', 'an', 'ours', 'perhaps', 'only', 'five', 'towards', 'keep', 'eleven', 'one', 'other', 'any', 'otherwise', 'except', 'that', 'cannot', 'behind', 'ourselves', 'under', 'within', 'fifty', 'across', 'if', 'thus', 'per', 'wherein', 'here', 'empty', 'co', 'still', 'whole', 'how', 'off', 'to', 'yourself', 'call', 'cry', 'four', 'so', 'she', 'take', 'their', 'been', 'now', 'even', 'mill', 'what', 'another', 'namely', 'always', 'themselves', 'almost', 'six', 'formerly', 'ten', 'found', 'onto', 'yet', 'between', 'give', 'hers', 'herein', 'eight', 'above', 'anyway', 'third', 'himself', 'front', 'over', 'two', 'much', 'latter', 'itself', 'besides', 'those', 'on', 'twenty', 'up', 'us', 'amongst', 'beforehand', 'but', 'most', 'same', 'mine', 'should', 'this', 'full', 'herself', 'her', 'thick', 'con', 'everything', 'is', 'am', 'three', 'throughout', 'again', 'enough', 'your', 'once', 'hereupon', 'become', 'yourselves', 'everyone', 'before', 'i', 'whereby', 'others', 'must', 'seems', 'elsewhere', 'were', 'either', 'would', 'became', 'hundred', 'toward', 'very', 'latterly', 'top', 'often', 'beside', 'cant', 'else', 'the', 'however', 'and', 'somehow', 'him', 'noone', 'somewhere', 'our', 'nothing', 'de', 'fill', 'well', 'it', 'all', 'last', 'do', 'these', 'has', 'upon', 'every', 'side', 'system', 'put', 'thence', 'twelve', 'becoming', 'show', 'un', 'least', 'of', 'have', 'own', 'since', 'though', 'whither', 'out', 'hereby', 'meanwhile', 'none', 'while', 'whom', 'further', 'why', 'made', 'whose', 'my', 'someone', 'they', 'during', 'anyone', 'first', 'go', 'less', 'his', 'anything', 'thereby', 'amount', 'together', 'never', 'was', 'thin', 'also', 'each', 'fire', 'are', 'when', 'alone', 'had', 'until', 'done', 'more', 'at', 'than', 'nowhere', 'seem', 'whence', 'name', 'neither', 'he', 'get', 'being', 'bottom'}\n",
    "    #strip and change to lower case and replace commas and semi colons with spaces\n",
    "    stem = string.strip().lower().replace(';', ' ').replace(',', ' ').replace(':', ' ').replace('(',' ').replace(')',' ').replace('#', ' ').replace('.', ' ').strip()\n",
    "         \n",
    "    # remove words that only have numbers( second one removes special characters also)\n",
    "    stem = re.sub(r'\\b\\d+\\b', ' ',stem).strip()\n",
    "    \n",
    "    #remove special characters at the end of words\n",
    "    stem = re.sub(r'([^\\w\\s]|_)+(?=\\s|$)', ' ',stem).strip()\n",
    "    \n",
    "    #remove any words that have a number in it ( even if in the middle )\n",
    "    stem = re.sub(r'\\w*\\d\\w*', ' ',stem).strip()\n",
    "    \n",
    "    # remove any words with only one alphabet\n",
    "    stem = re.sub(r'\\b[a-zA-Z]\\b', ' ',stem).strip()\n",
    "    \n",
    "    # remove stop words and change to lower case\n",
    "    stem = ' '.join([item for item in (stem.strip().split()) if item not in stopwords])\n",
    "    \n",
    "    # get words greater than length 4 and less than 25\n",
    "    stem = ' '.join([item for item in (re.findall('\\w{4,25}', stem))])\n",
    "    #re.findall('\\w{4,25}', stem).join(' ').strip() \n",
    "    \n",
    "     #apply stemming for all words\n",
    "    stem = tokenize_and_stem(stem)\n",
    "    return stem\n",
    "\n",
    "#query for first level cluster and then second level cluster\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction import text\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import nltk\n",
    "from string import printable\n",
    "sb_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "my_words = ['','()','(),']\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(my_words)\n",
    "\n",
    "def tokenize(doc):\n",
    "    return doc.lower().split(\" \")\n",
    "\n",
    "def query_first_level_cluster(query_string):\n",
    "    #find first level cluster\n",
    "    nmf_filename = '/home/ubuntu/uspto/nmfmodels/nmf_model_X.sav'\n",
    "    tfidf_filename = '/home/ubuntu/uspto/tfidfmodels/tfidf_model_X.sav'\n",
    "    # load the model from disk\n",
    "    loaded_nmf_model = pickle.load(open(nmf_filename, 'rb'))\n",
    "    loaded_tfidf_model = pickle.load(open(tfidf_filename, 'rb'))\n",
    "\n",
    "    query_string_pr_stem = clean_stem(query_string)\n",
    "\n",
    "    query_tfidf = loaded_tfidf_model.transform([query_string_pr_stem])\n",
    "    nmf_query = loaded_nmf_model.transform(query_tfidf)\n",
    "    # print(nmf_query)\n",
    "    # nmf_query.argmax(axis=1)\n",
    "    return(nmf_query.argmax(axis=1)[0])\n",
    "\n",
    "def query_second_level_cluster(query_string, first_level_cluster):\n",
    "    #find second level cluster\n",
    "    nmf_filename = '/home/ubuntu/uspto/nmfmodels/nmf_model_' + str(first_level_cluster) + '.sav'\n",
    "    tfidf_filename = '/home/ubuntu/uspto/tfidfmodels/tfidf_model_' + str(first_level_cluster) + '.sav' \n",
    "    # load the model from disk\n",
    "    loaded_nmf_model = pickle.load(open(nmf_filename, 'rb'))\n",
    "    loaded_tfidf_model = pickle.load(open(tfidf_filename, 'rb'))\n",
    "    \n",
    "    query_string_pr_stem = clean_stem(query_string)\n",
    "    \n",
    "    query_tfidf = loaded_tfidf_model.transform([query_string_pr_stem])\n",
    "    nmf_query = loaded_nmf_model.transform(query_tfidf)\n",
    "    # print(nmf_query)\n",
    "    # nmf_query.argmax(axis=1)\n",
    "    return(nmf_query.argmax(axis=1)[0])\n",
    "\n",
    "def query_similarities(query_string, first_level_cluster,second_level_cluster,cosine_sim_threshold,top):\n",
    "    #find top similarities for a given cluster combo\n",
    "    \n",
    "    filename = '/home/ubuntu/uspto/smalldatafiles/small_cluster_data_' + str(first_level_cluster) + '_' + str(second_level_cluster) + '.csv'\n",
    "    tfidf_model_filename = '/home/ubuntu/uspto/tfidfmodels/tfidf_model_' + str(first_level_cluster) + '_' + str(second_level_cluster) + '.sav'\n",
    "    tfidf_filename = '/home/ubuntu/uspto/tfidfmodels/tfidf_' + str(first_level_cluster) + '_' + str(second_level_cluster) + '.sav'\n",
    "\n",
    "    df_data = pd.read_csv(filename,usecols = ['appl_doc_number', 'appl_doc_number', 'invention_title','abstract','claim_text','patent_number'])\n",
    "    loaded_tfidf_vectorizer = pickle.load(open(tfidf_model_filename, 'rb'))\n",
    "    loaded_tfidf = pickle.load(open(tfidf_filename, 'rb'))\n",
    "    \n",
    "    \n",
    "    query_tfidf = loaded_tfidf_vectorizer.transform([query_string])\n",
    "\n",
    "    df_result = pd.DataFrame(columns=['cosine_similarity', 'appl_doc_number','invention_title','abstract','claim_text','patent_number'])\n",
    "\n",
    "    # get the result and add to a new dataframe\n",
    "    loc_index = 0\n",
    "    patent_query = query_tfidf.toarray()[0]\n",
    "    for index_corpus, patent_corpus in enumerate(loaded_tfidf.toarray()):\n",
    "        #cosine_sim = cosine_similarity(patent_query, patent_corpus)\n",
    "        cosine_sim = 1 - spatial.distance.cosine(patent_query, patent_corpus)\n",
    "        if cosine_sim > cosine_sim_threshold:\n",
    "            #res_row = df_data.iloc[index_corpus]\n",
    "            df_result.loc[loc_index] = [cosine_sim,df_data.iloc[index_corpus]['appl_doc_number'],df_data.iloc[index_corpus]['invention_title'],df_data.iloc[index_corpus]['abstract'],df_data.iloc[index_corpus]['claim_text'],df_data.iloc[index_corpus]['patent_number']]\n",
    "            #df_result.loc[loc_index] = [cosine_sim,res_row['appl_doc_number'],res_row['invention_title'],res_row['abstract'],res_row['claim_text'],res_row['patent_number']]\n",
    "            \n",
    "            loc_index += 1\n",
    "\n",
    "    # sort the results and pick top results        \n",
    "    df_result = df_result.sort_values('cosine_similarity',ascending=[False])\n",
    "       \n",
    "    df_result = df_result[:top]\n",
    "    \n",
    "    #remove any non-printable characters\n",
    "    df_result['abstract'] = df_result['abstract'].apply(lambda x: re.sub(r'[^\\x00-\\x7f]',r'', x))\n",
    "    df_result['abstract'] = df_result['abstract'].apply(lambda x: x.replace(\" &#8216\", '').replace(\" &#8217\", ''))\n",
    "    df_result['claim_text'] = df_result['claim_text'].apply(lambda x: re.sub(r'[^\\x00-\\x7f]',r'', x))\n",
    "    df_result['claim_text'] = df_result['claim_text'].apply(lambda x: x.replace(\"&#8216;\", ' ').replace(\"&#8217;\", ' '))\n",
    " \n",
    "    return(df_result)\n",
    "\n",
    "def query_wrapper(query_string,cosine_sim_threshold,top):\n",
    "    first_cluster = query_first_level_cluster(query_string)\n",
    "    second_cluster = query_second_level_cluster(query_string,first_cluster)\n",
    "    df_result = query_similarities(query_string,first_cluster,second_cluster,cosine_sim_threshold,top)\n",
    "    return df_result,first_cluster,second_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.csv( path = \"file:///home/ubuntu/uspto/smalldatafiles/small_cluster_data_*\",header = True,inferSchema = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.select(\"claim_text\").filter(df[\"appl_doc_number\"] == 13698956).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = pd.read_csv(\"/home/ubuntu/validationdata/priorart_data_split_2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "num = list(recs['application_number'])\n",
    "\n",
    "scores = []\n",
    "toCSV = []\n",
    "csvwrite = {}\n",
    "idx = 1\n",
    "for app in set(num):\n",
    "    if idx > 1:\n",
    "        break\n",
    "    query_string = df.select(\"claim_text\").filter(df[\"appl_doc_number\"] == app).collect()\n",
    "    result = query_wrapper(query_string[0],0.3,7)\n",
    "    lista = list(result['patent_number'])\n",
    "    csvwrite['model_result'] = lista\n",
    "    \n",
    "    listb = list(recs[recs['application_number'] == app]['priorart_number'])\n",
    "    csvwrite['compare_with'] = listb\n",
    "    csvwrite['score']=similar(lista, listb)\n",
    "    toCSV.append(csvwrite)\n",
    "    scores.append(csvwrite['score'])\n",
    "    idx = idx + 1\n",
    "ns = np.array(scores)\n",
    "ns.mean() * 100\n",
    "keys = toCSV[0].keys()\n",
    "with open('/home/ubuntu/uspto/results.csv', 'wb') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(toCSV)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

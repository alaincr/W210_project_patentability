{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard python helper libraries.\n",
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "import scipy.optimize\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "# Helper libraries (see the corresponding py files in this notebook's directory).\n",
    "import segment\n",
    "import utils\n",
    "import vocabulary\n",
    "#import helpers\n",
    "\n",
    "\n",
    "# Plotly imports.\n",
    "import plotly.offline as plotly\n",
    "plotly.offline.init_notebook_mode()\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "import codecs\n",
    "# This runs a shell command from the notebook.\n",
    "!pip install plotly\n",
    "\n",
    "import collections\n",
    "from itertools import chain\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "##################################################################################################################\n",
    "# Some basic stipping out to reduce vocabulary size\n",
    "##################################################################################################################\n",
    "# Strip out punctuations\n",
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "punctuations.append(\"''\")\n",
    "\n",
    "\n",
    "# Function to stip out non-ASIC characters\n",
    "def strip_non_ascii(string):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in string if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "\n",
    "\n",
    "############################################################################################################################\n",
    "# Actual code start\n",
    "############################################################################################################################\n",
    "\n",
    "path = \"CSV/*.csv\"\n",
    "for file_name in glob.glob(path):\n",
    "    df_full = pd.read_csv(file_name)\n",
    "    print file_name\n",
    "    df_full = df_full[df_full['claim_text_stemmed'].notnull()]\n",
    "    df_full = df_full[df_full['appl_doc_number'].notnull()]    \n",
    "    df_full = df_full.drop_duplicates(subset=[ 'claim_text_stemmed'], keep=False)\n",
    "    # Get the number of sentences\n",
    "    claims_range = len(df_full)\n",
    "    df_all = df_full    \n",
    "\n",
    "    # Get all the claims to create a corpus\n",
    "    claims_array = []\n",
    "    # Stip out items in the claims array\n",
    "    for i in range(claims_range):\n",
    "        claim = df_full.iloc[i]['claim_text_stemmed']\n",
    "        # Remove non-ASCII characters\n",
    "        claim=strip_non_ascii(claim)\n",
    "        # Strip out punctuations\n",
    "        claim = [i for i in nltk.word_tokenize(claim) if i not in punctuations]\n",
    "        # Put them back as a sentence again\n",
    "        claim = \" \".join(claim)\n",
    "        claims_array.append(claim)\n",
    "\n",
    "    # We are going to examine the comments for some statistics\n",
    "    # Get the words\n",
    "    #tokens = list(utils.flatten(s.split() for s in claims_array))\n",
    "\n",
    "    # \"canonicalize_word\" performs a few tweaks to the token stream of\n",
    "    # the corpus.  For example, it replaces digits with DG allowing numbers\n",
    "    # to aggregate together when we count them below.\n",
    "\n",
    "    #token_feed = (utils.canonicalize_word(w) for w in tokens)\n",
    "    #vocab = vocabulary.Vocabulary(token_feed)\n",
    "    #print file_name, \"Vocabulary size:\", vocab.size\n",
    "\n",
    "    # Get the array range \n",
    "    final_claims_range = len(claims_array)\n",
    "\n",
    "    new_claims_array = claims_array\n",
    "    new_claims_range = len(new_claims_array)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Doc2vec section\n",
    "    doc2vec_words = []\n",
    "    for f in range(new_claims_range):\n",
    "        doc2vec_words.append(new_claims_array[f])\n",
    "        doc2vec_words.append('\\n')\n",
    "    \n",
    "\n",
    "    doc2vec_words = list(chain.from_iterable(doc2vec_words))\n",
    "    doc2vec_words = ''.join(doc2vec_words)[:-1]\n",
    "\n",
    "    # Build the sentences\n",
    "    doc2vec_sentences = doc2vec_words.split('\\n')\n",
    "    \n",
    "\n",
    "\n",
    "    cpus = cpu_count()\n",
    "\n",
    "    def read_corpus():\n",
    "        for i,sentence in enumerate(doc2vec_words.split('\\n')):\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(sentence), [i])\n",
    "\n",
    "    train_corpus = list(read_corpus())\n",
    "\n",
    "    embedding_size = 500 # Dimension of the embedding vector.\n",
    "    window = 8\n",
    "\n",
    "    doc2vec_model = Doc2Vec(dm=1, dm_concat=1, size=embedding_size, window=window, \n",
    "                    negative=0,hs=0, min_count=1, workers=cpus, iter=10)\n",
    "    doc2vec_model.build_vocab(train_corpus)\n",
    "\n",
    "    #doc2vec_model.save('small_doc2vec_model')   \n",
    "    #doc2vec_model = Doc2Vec.load('small_doc2vec_model')\n",
    "    \n",
    "    %%time\n",
    "    doc2vec_model.train(train_corpus, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.iter)\n",
    "    doc2vec_model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    train = file_name.split(\"/\")\n",
    "    train = train[1].split(\".\")\n",
    "\n",
    "    train_model = 'MODELS/'+train[0]+'_train'\n",
    "    print \"Writing\", train_model\n",
    "    doc2vec_model.save(train_model)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ncw210data/FullTextData/patent_claim_data_all_years.csv to ../data/inputdata/patent_claim_data_all_years.csv\n"
     ]
    }
   ],
   "source": [
    "#get the data file with all years data from S3 for clustering\n",
    "!aws s3 cp s3://ncw210data/FullTextData/patent_claim_data_all_years.csv /home/ubuntu/data/inputdata/patent_claim_data_all_years.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appl_doc_number,appl_country,appl_date,pub_doc_number,pub_date,number_of_days,invention_title,abstract,claim_text,claim_text_stemmed\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 /home/ubuntu/data/inputdata/patent_claim_data_all_years.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import NMF\n",
    "import csv\n",
    "\n",
    "#clustering and small files creation\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sb_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "stopwords = {'back', 'thru', 'eg', 'hereafter', 'too', 'part', 'which', 'will', 'be', 'thereupon', 'about', 'nevertheless', 'therein', 'through', 'we', 'among', 'in', 'then', 'former', 'via', 'below', 'whereafter', 'due', 'you', 'bill', 'forty', 'few', 'not', 'with', 'rather', 'next', 'nine', 'me', 'its', 'sometime', 'yours', 'who', 'whoever', 'down', 'some', 'such', 'thereafter', 'hasnt', 'fifteen', 'both', 'as', 'ever', 'could', 'find', 'hence', 'something', 'a', 'there', 'mostly', 'whereas', 'many', 'serious', 'can', 'indeed', 'afterwards', 'whenever', 'by', 'becomes', 'may', 'after', 'couldnt', 'seemed', 'anyhow', 'etc', 'might', 'already', 'no', 'please', 'them', 'myself', 'therefore', 'from', 'along', 'ltd', 'against', 'everywhere', 'amoungst', 'because', 'where', 'sixty', 'ie', 'although', 'sincere', 'move', 'seeming', 'or', 'wherever', 'inc', 'whatever', 'into', 'anywhere', 'around', 'nor', 'see', 'several', 'sometimes', 'for', 'interest', 'beyond', 'whether', 'detail', 'describe', 'moreover', 'nobody', 'whereupon', 're', 'without', 'an', 'ours', 'perhaps', 'only', 'five', 'towards', 'keep', 'eleven', 'one', 'other', 'any', 'otherwise', 'except', 'that', 'cannot', 'behind', 'ourselves', 'under', 'within', 'fifty', 'across', 'if', 'thus', 'per', 'wherein', 'here', 'empty', 'co', 'still', 'whole', 'how', 'off', 'to', 'yourself', 'call', 'cry', 'four', 'so', 'she', 'take', 'their', 'been', 'now', 'even', 'mill', 'what', 'another', 'namely', 'always', 'themselves', 'almost', 'six', 'formerly', 'ten', 'found', 'onto', 'yet', 'between', 'give', 'hers', 'herein', 'eight', 'above', 'anyway', 'third', 'himself', 'front', 'over', 'two', 'much', 'latter', 'itself', 'besides', 'those', 'on', 'twenty', 'up', 'us', 'amongst', 'beforehand', 'but', 'most', 'same', 'mine', 'should', 'this', 'full', 'herself', 'her', 'thick', 'con', 'everything', 'is', 'am', 'three', 'throughout', 'again', 'enough', 'your', 'once', 'hereupon', 'become', 'yourselves', 'everyone', 'before', 'i', 'whereby', 'others', 'must', 'seems', 'elsewhere', 'were', 'either', 'would', 'became', 'hundred', 'toward', 'very', 'latterly', 'top', 'often', 'beside', 'cant', 'else', 'the', 'however', 'and', 'somehow', 'him', 'noone', 'somewhere', 'our', 'nothing', 'de', 'fill', 'well', 'it', 'all', 'last', 'do', 'these', 'has', 'upon', 'every', 'side', 'system', 'put', 'thence', 'twelve', 'becoming', 'show', 'un', 'least', 'of', 'have', 'own', 'since', 'though', 'whither', 'out', 'hereby', 'meanwhile', 'none', 'while', 'whom', 'further', 'why', 'made', 'whose', 'my', 'someone', 'they', 'during', 'anyone', 'first', 'go', 'less', 'his', 'anything', 'thereby', 'amount', 'together', 'never', 'was', 'thin', 'also', 'each', 'fire', 'are', 'when', 'alone', 'had', 'until', 'done', 'more', 'at', 'than', 'nowhere', 'seem', 'whence', 'name', 'neither', 'he', 'get', 'being', 'bottom'}\n",
    "\n",
    "def tokenize(doc):\n",
    "    return doc.lower().split(\" \")\n",
    "\n",
    "def save_topics(model, feature_names, no_top_words,topics_filename):\n",
    "    with open(topics_filename, 'w') as csvfile:\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            csvwriter = csv.writer(csvfile, delimiter=',',quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            words = (\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "            #print(topic_idx, (\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])))\n",
    "            csvwriter.writerow([topic_idx, words])\n",
    "\n",
    "def create_clusters(filename,level,cluster_number):\n",
    "    #df_data = pd.read_csv(filename,usecols=['application_number','uspc_class','uspc_subclass','claim_text_pr_stem'])\n",
    "    #['appl_doc_number', 'appl_country', 'appl_date', 'pub_doc_number', 'pub_date', 'number_of_days','invention_title', 'abstract','claim_text','claim_text_stemmed']\n",
    "    df_data = pd.read_csv(filename,encoding = 'utf-8')\n",
    "    print('file loaded')\n",
    "    df_data = df_data.drop_duplicates(subset=[ 'appl_doc_number'], keep=False)\n",
    "    df_data = df_data.dropna(subset=['claim_text_stemmed'], how='all')\n",
    "    \n",
    "    my_words = ['','()','(),']\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(my_words)\n",
    " \n",
    "    corpus_vectorizer = TfidfVectorizer(norm='l2', max_df=0.95, min_df=2, use_idf=True, smooth_idf=False, sublinear_tf=True,\n",
    "                                    stop_words=set(my_stop_words),\n",
    "                                    tokenizer=tokenize)\n",
    "\n",
    "    corpus_tfidf = corpus_vectorizer.fit_transform(df_data['claim_text_stemmed'])\n",
    "    print('vectorizer fit transform completed')\n",
    "    \n",
    "    no_topics = 10\n",
    "\n",
    "    # Run NMF\n",
    "    nmfmodel = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "    H = nmfmodel.fit_transform(corpus_tfidf)\n",
    "    nmf = nmfmodel.components_;\n",
    "    print('nmf fit transform completed')\n",
    "    \n",
    "    tfidf_feature_names = corpus_vectorizer.get_feature_names()\n",
    "\n",
    "    num_top_words = 2000\n",
    "    fname = '/home/ubuntu/data/clusterdata/topics_' + str(cluster_number) + '.csv'\n",
    "    save_topics(nmfmodel, tfidf_feature_names, num_top_words, fname)\n",
    "\n",
    "    # save the model to disk\n",
    "    nmf_filename = '/home/ubuntu/data/clusterdata/nmf_model_' + str(cluster_number) + '.sav'\n",
    "    pickle.dump(nmfmodel, open(nmf_filename, 'wb'))\n",
    "    \n",
    "    tfidf_filename = '/home/ubuntu/data/clusterdata/tfidf_model_' + str(cluster_number) + '.sav' \n",
    "    pickle.dump(corpus_vectorizer, open(tfidf_filename, 'wb'))\n",
    "    \n",
    "    print('models saved')\n",
    "    \n",
    "    #cluster_H_cols = np.around(np.matrix(H),decimals=4)\n",
    "    #cluster_H_rank = np.array([cluster_H_cols.argmax(axis=1)])\n",
    "    #cluster_H = np.concatenate((cluster_H_rank.T, cluster_H_cols), axis=1)\n",
    "\n",
    "    cluster_proba_cols = np.matrix(H) * 100 /np.matrix(H).sum(axis=1)\n",
    "    cluster_proba_cols = np.around(cluster_proba_cols,decimals=4)\n",
    "    cluster_proba_rank = np.array([cluster_proba_cols.argmax(axis=1)])\n",
    "    cluster_proba = np.concatenate((cluster_proba_rank.T, cluster_proba_cols), axis=1)\n",
    "\n",
    "    np_appl = np.matrix(df_data[['appl_doc_number', 'appl_country', 'appl_date', 'pub_doc_number', 'pub_date', 'number_of_days','invention_title', 'abstract','claim_text','claim_text_stemmed']])\n",
    "\n",
    "    # np_appl_H = np.concatenate((np_appl, cluster_H), axis=1)\n",
    "    np_appl_proba = np.concatenate((np_appl, cluster_proba), axis=1)\n",
    "\n",
    "    H_filename = '/home/ubuntu/data/clusterdata/all_cluster_data_H_' + str(cluster_number) + '.csv'\n",
    "    probs_filename = '/home/ubuntu/data/clusterdata/all_cluster_data_probs_' + str(cluster_number) + '.csv'\n",
    "    \n",
    "    # np.savetxt(H_filename,np_appl_H, \n",
    "    #delimiter=\",\",fmt='%s,%s,%s,%s,%i,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f')\n",
    "    \n",
    "    #numpy savetxt was giving lot of encoding errors so had to convert to pandas dataframe before writing to csv\n",
    "    #np.savetxt(probs_filename,np_appl_proba,\n",
    "    #delimiter=\",\",fmt='%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%i,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f,%3.4f')\n",
    "    df_probs = pd.DataFrame(np_appl_proba)\n",
    "    df_probs.to_csv(probs_filename,index=False, sep=',')\n",
    "       \n",
    "    print('Probs saved')\n",
    "    \n",
    "    # split the probs file to small cluster files\n",
    "    chunksize = 100000\n",
    "    count = 0\n",
    "    for df_data in pd.read_csv(probs_filename, sep = ',', header=None,encoding = \"ISO-8859-1\", chunksize=chunksize):\n",
    "    #for df_data in pd.read_csv(probs_filename, sep = ',', header=None, encoding = \"ISO-8859-1\", chunksize=chunksize):\n",
    "        df_data.columns = ['appl_doc_number', 'appl_country', 'appl_date', 'pub_doc_number', 'pub_date', 'number_of_days','invention_title', 'abstract','claim_text','claim_text_stemmed','top_cluster', 'pr_0','pr_1','pr_2','pr_3','pr_4','pr_5','pr_6','pr_7','pr_8','pr_9']\n",
    "\n",
    "        filename = '/home/ubuntu/data/clusterdata/small_cluster_data_' + str(level) \n",
    "\n",
    "        if count == 0:\n",
    "            df_data[df_data['top_cluster'] == 0].to_csv(filename + '_0.csv',index=False, sep=',') \n",
    "            df_data[df_data['top_cluster'] == 1].to_csv(filename + '_1.csv',index=False, sep=',') \n",
    "            df_data[df_data['top_cluster'] == 2].to_csv(filename + '_2.csv',index=False, sep=',') \n",
    "            df_data[df_data['top_cluster'] == 3].to_csv(filename + '_3.csv',index=False, sep=',') \n",
    "            df_data[df_data['top_cluster'] == 4].to_csv(filename + '_4.csv',index=False, sep=',') \n",
    "            df_data[df_data['top_cluster'] == 5].to_csv(filename + '_5.csv',index=False, sep=',') \n",
    "            df_data[df_data['top_cluster'] == 6].to_csv(filename + '_6.csv',index=False, sep=',') \n",
    "            df_data[df_data['top_cluster'] == 7].to_csv(filename + '_7.csv',index=False, sep=',') \n",
    "            df_data[df_data['top_cluster'] == 8].to_csv(filename + '_8.csv',index=False, sep=',') \n",
    "            df_data[df_data['top_cluster'] == 9].to_csv(filename + '_9.csv',index=False, sep=',') \n",
    "            count += 1\n",
    "        else:    \n",
    "            df_data[df_data['top_cluster']== 0].to_csv(filename + '_0.csv',mode = 'a', header= False, index=False, sep=',')\n",
    "            df_data[df_data['top_cluster']== 1].to_csv(filename + '_1.csv',mode = 'a', header= False, index=False, sep=',')\n",
    "            df_data[df_data['top_cluster']== 2].to_csv(filename + '_2.csv',mode = 'a', header= False, index=False, sep=',')\n",
    "            df_data[df_data['top_cluster']== 3].to_csv(filename + '_3.csv',mode = 'a', header= False, index=False, sep=',')\n",
    "            df_data[df_data['top_cluster']== 4].to_csv(filename + '_4.csv',mode = 'a', header= False, index=False, sep=',')\n",
    "            df_data[df_data['top_cluster']== 5].to_csv(filename + '_5.csv',mode = 'a', header= False, index=False, sep=',')\n",
    "            df_data[df_data['top_cluster']== 6].to_csv(filename + '_6.csv',mode = 'a', header= False, index=False, sep=',')\n",
    "            df_data[df_data['top_cluster']== 7].to_csv(filename + '_7.csv',mode = 'a', header= False, index=False, sep=',')\n",
    "            df_data[df_data['top_cluster']== 8].to_csv(filename + '_8.csv',mode = 'a', header= False, index=False, sep=',')\n",
    "            df_data[df_data['top_cluster']== 9].to_csv(filename + '_9.csv',mode = 'a', header= False, index=False, sep=',')\n",
    "    print('small files created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file loaded\n",
      "vectorizer fit transform completed\n",
      "nmf fit transform completed\n",
      "models saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:74: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probs saved\n",
      "small files created\n"
     ]
    }
   ],
   "source": [
    "#first level clustering\n",
    "filename = '/home/ubuntu/data/inputdata/patent_claim_data_all_years.csv'\n",
    "level = 'X'\n",
    "cluster_number = 'X'\n",
    "\n",
    "create_clusters(filename, level, cluster_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results_pr_stem = set()\n",
    "# df_data['claim_text_pr_stem'].str.lower().str.split().apply(results_pr_stem.update)\n",
    "# print(len(results_pr_stem))\n",
    "# #print(results_pr_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/clusterdata/small_cluster_data_X_0.csv\n",
      "file loaded\n",
      "vectorizer fit transform completed\n",
      "nmf fit transform completed\n",
      "models saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:74: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probs saved\n",
      "small files created\n",
      "/home/ubuntu/data/clusterdata/small_cluster_data_X_1.csv\n",
      "file loaded\n",
      "vectorizer fit transform completed\n",
      "nmf fit transform completed\n",
      "models saved\n",
      "Probs saved\n",
      "small files created\n",
      "/home/ubuntu/data/clusterdata/small_cluster_data_X_2.csv\n",
      "file loaded\n",
      "vectorizer fit transform completed\n",
      "nmf fit transform completed\n",
      "models saved\n",
      "Probs saved\n",
      "small files created\n",
      "/home/ubuntu/data/clusterdata/small_cluster_data_X_3.csv\n",
      "file loaded\n",
      "vectorizer fit transform completed\n",
      "nmf fit transform completed\n",
      "models saved\n",
      "Probs saved\n",
      "small files created\n",
      "/home/ubuntu/data/clusterdata/small_cluster_data_X_4.csv\n",
      "file loaded\n",
      "vectorizer fit transform completed\n",
      "nmf fit transform completed\n",
      "models saved\n",
      "Probs saved\n",
      "small files created\n",
      "/home/ubuntu/data/clusterdata/small_cluster_data_X_5.csv\n",
      "file loaded\n",
      "vectorizer fit transform completed\n",
      "nmf fit transform completed\n",
      "models saved\n",
      "Probs saved\n",
      "small files created\n",
      "/home/ubuntu/data/clusterdata/small_cluster_data_X_6.csv\n",
      "file loaded\n",
      "vectorizer fit transform completed\n",
      "nmf fit transform completed\n",
      "models saved\n",
      "Probs saved\n",
      "small files created\n",
      "/home/ubuntu/data/clusterdata/small_cluster_data_X_7.csv\n",
      "file loaded\n",
      "vectorizer fit transform completed\n",
      "nmf fit transform completed\n",
      "models saved\n",
      "Probs saved\n",
      "small files created\n",
      "/home/ubuntu/data/clusterdata/small_cluster_data_X_8.csv\n",
      "file loaded\n",
      "vectorizer fit transform completed\n",
      "nmf fit transform completed\n",
      "models saved\n",
      "Probs saved\n",
      "small files created\n",
      "/home/ubuntu/data/clusterdata/small_cluster_data_X_9.csv\n",
      "file loaded\n",
      "vectorizer fit transform completed\n",
      "nmf fit transform completed\n",
      "models saved\n",
      "Probs saved\n",
      "small files created\n"
     ]
    }
   ],
   "source": [
    "#second level clustering\n",
    "filepath = '/home/ubuntu/data/clusterdata/small_cluster_data_X_'\n",
    "for i in range(0, 10):\n",
    "    filename = filepath + str(i) + '.csv'\n",
    "    cluster_number = str(i)\n",
    "    level = str(i)\n",
    "    print(filename)\n",
    "    create_clusters(filename, level, cluster_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to create tfidf models for second level clustered files\n",
    "def create_tfidf_for_small_files(filename,p,c):\n",
    "    df_data = pd.read_csv(filename,usecols=['appl_doc_number', 'appl_country', 'appl_date', 'pub_doc_number', 'pub_date', 'number_of_days','invention_title', 'abstract','claim_text','claim_text_stemmed'])\n",
    "    df_data = df_data.drop_duplicates(subset=[ 'appl_doc_number'], keep=False)\n",
    "    df_data = df_data.dropna(subset=['claim_text_stemmed'], how='all')\n",
    "    \n",
    "    my_words = ['','()','(),']\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(my_words)\n",
    " \n",
    "    corpus_vectorizer = TfidfVectorizer(norm='l2', max_df=0.95, min_df=2, use_idf=True, smooth_idf=False, sublinear_tf=True,\n",
    "                                    stop_words=set(my_stop_words),\n",
    "                                    tokenizer=tokenize)\n",
    "\n",
    "    corpus_tfidf = corpus_vectorizer.fit_transform(df_data['claim_text_stemmed'])\n",
    "    \n",
    "    tfidf_filename = '/home/ubuntu/data/clusterdata/tfidf_model_' + str(p) + '_' +str(c) + '.sav' \n",
    "    pickle.dump(corpus_vectorizer, open(tfidf_filename, 'wb'))\n",
    "    #pickle.dump(corpus_vectorizer.vocabulary_, open(tfidf_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create tfidf models for second level clustered files\n",
    "for p in range(0,10):\n",
    "    for c in range(0,10):\n",
    "        filename = '/home/ubuntu/data/clusterdata/small_cluster_data_'+ str(p) + '_' +str(c) + '.csv' \n",
    "        create_tfidf_for_small_files(filename,p,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#copy all files to S3 ( this is only a backup with all intermediate files)\n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/  s3://ncw210data/FullTextDataClusters/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#copy all files to S3 (to be used by application and follow up models)\n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/topics --recursive --exclude \"*\" --include \"topics_*.csv\"\n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/tfidfmodels --recursive --exclude \"*\" --include \"tfidf*.sav\" \n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/nmfmodels --recursive --exclude \"*\" --include \"nmf*.sav\" \n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/smalldatafiles --recursive --exclude \"*\" --include \"small_cluster_data_0*.csv\"\n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/smalldatafiles --recursive --exclude \"*\" --include \"small_cluster_data_1*.csv\"    \n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/smalldatafiles --recursive --exclude \"*\" --include \"small_cluster_data_2*.csv\"    \n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/smalldatafiles --recursive --exclude \"*\" --include \"small_cluster_data_3*.csv\"    \n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/smalldatafiles --recursive --exclude \"*\" --include \"small_cluster_data_4*.csv\"    \n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/smalldatafiles --recursive --exclude \"*\" --include \"small_cluster_data_5*.csv\"\n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/smalldatafiles --recursive --exclude \"*\" --include \"small_cluster_data_6*.csv\"    \n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/smalldatafiles --recursive --exclude \"*\" --include \"small_cluster_data_7*.csv\"    \n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/smalldatafiles --recursive --exclude \"*\" --include \"small_cluster_data_8*.csv\"    \n",
    "# !aws s3 cp /home/ubuntu/data/clusterdata/ s3://ncw210data/NewClusterFiles/smalldatafiles --recursive --exclude \"*\" --include \"small_cluster_data_9*.csv\"    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
